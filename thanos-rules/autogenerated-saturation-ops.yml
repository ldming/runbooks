# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: Saturation Rules (autogenerated)
  interval: 1m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio
    labels:
      component: go_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard, fqdn) (
              go_memstats_alloc_bytes{env="ops",type=~"gitaly|monitoring|praefect|registry|web-pages"}
            )
            /
            sum by (env, environment, tier, type, stage, shard, fqdn) (
              node_memory_MemTotal_bytes{env="ops",type=~"gitaly|monitoring|praefect|registry|web-pages"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_go_memory
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            go_memstats_alloc_bytes{env="ops",type=~"api|git|internal-api|kas|monitoring|registry|vault|web|web-pages|websockets"}
            / on(env, environment, tier, type, stage, shard, cluster, pod) group_left()
            topk by(env, environment, tier, type, stage, shard, cluster, pod) (1,
              container_spec_memory_limit_bytes:labeled{container=~"gitlab-pages|gitlab-workhorse|kas|registry|thanos-store",env="ops",type=~"api|git|internal-api|kas|monitoring|registry|vault|web|web-pages|websockets"}
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_node_ips
      stage: main
      tier: inf
      type: kube
    expr: |
      max by(env, environment, shard) (
        clamp_min(
          clamp_max(
            sum(:kube_pod_info_node_count:{env="ops"}) by (env, environment, shard, cluster)
            /
            sum(
              gitlab:gcp_subnet_max_ips{env="ops"} * on (subnet) group_right gitlab:cluster:subnet:mapping{env="ops"}
            ) by (env, environment, shard, cluster)
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: kube_pool_max_nodes
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
              kube_node_labels:labeled{env="ops",type=~"api|internal-api|git|kube|redis-cache|redis-sidekiq|redis-tracechunks|redis-ratelimiting|redis-cluster-ratelimiting|redis-sessions|redis-registry-cache|redis-repository-cache|redis-db-load-balancing|redis|registry|sidekiq|vault|web-pages|web|websockets"}
            )
            / on(cluster, env, environment, label_pool) group_left() (
              label_replace(
                terraform_report_google_cluster_node_pool_max_node_count,
                "label_pool", "$0", "pool_name", ".*"
              )
              * on(cluster, env, environment) group_left()
              count by (cluster, env, environment) (
                group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                  kube_node_labels:labeled{env="ops",type=~"api|internal-api|git|kube|redis-cache|redis-sidekiq|redis-tracechunks|redis-ratelimiting|redis-cluster-ratelimiting|redis-sessions|redis-registry-cache|redis-repository-cache|redis-db-load-balancing|redis|registry|sidekiq|vault|web-pages|web|websockets"}
                )
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_primary_cpu
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            avg without(cpu, mode) (
              1
              -
              (
                rate(node_cpu_seconds_total{mode="idle", env="ops",type=~"patroni|patroni-registry|patroni-ci|postgres-archive|sentry"}[5m])
                and on(fqdn)
                pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-ci|postgres-archive|sentry"} == 0
              )
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_vacuum_activity_v2
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard) (
              rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-ci"}[1d])
              and on (fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-ci"} == 0)
            )
            /
            avg by (env, environment, tier, type, stage, shard) (
              pg_settings_autovacuum_max_workers{env="ops",type=~"patroni|patroni-registry|patroni-ci"}
              and on (instance, job) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-ci"} == 0)
            )
            ,
            1)
        ,
        0)
      )
  - record: gitlab_component_saturation:ratio
    labels:
      component: pg_walsender_cpu
    expr: |
      max by(env, environment, tier, type, stage, shard) (
        clamp_min(
          clamp_max(
            sum by (env, environment, tier, type, stage, shard) (
              sum by(env, environment, tier, type, stage, shard, fqdn) (
                rate(namedprocess_namegroup_cpu_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-ci", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                and on (fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-ci"} == 0)
              )
              /
              count by (env, environment, tier, type, stage, shard, fqdn) (
                node_cpu_seconds_total{env="ops",type=~"patroni|patroni-registry|patroni-ci", mode="idle"} and on(fqdn) (pg_replication_is_replica{env="ops",type=~"patroni|patroni-registry|patroni-ci"} == 0)
              )
            )
            ,
            1)
        ,
        0)
      )
- name: GitLab Component Saturation Statistics
  interval: 5m
  partial_response_strategy: warn
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{env="ops",monitor="global"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Go's memory allocation strategy can make it look like a Go process is saturating memory when measured using RSS, when in fact the process is not at risk of memory saturation. For this reason, we measure Go processes using the `go_memstat_alloc_bytes` metric instead of RSS.
      grafana_dashboard_id: alerts-sat_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_go_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3631721613"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              /
              sum by (env, environment, tier, type, stage, shard, fqdn) (
                node_memory_MemTotal_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="go_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="go_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Measures Go memory usage as a percentage of container memory limit
      grafana_dashboard_id: alerts-sat_kube_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_go_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4163523952"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, cluster, pod) (
          clamp_min(
            clamp_max(
              go_memstats_alloc_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              / on(env, environment, tier, type, stage, shard, cluster, pod) group_left()
              topk by(env, environment, tier, type, stage, shard, cluster, pod) (1,
                container_spec_memory_limit_bytes:labeled{container=~"gitlab-pages|gitlab-workhorse|kas|registry|thanos-store",environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_go_memory",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_go_memory",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Node IP subnet saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Node IP subnet saturation resource:

        This resource measures the number of nodes per subnet.

        If it is becoming saturated, it may indicate that clusters need to be rebuilt with a larger subnet.
      grafana_dashboard_id: alerts-sat_kube_node_ips
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_node_ips?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "776071338"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, shard, cluster) (
          clamp_min(
            clamp_max(
              sum(:kube_pod_info_node_count:{environment="{{ $labels.environment }}"}) by (env, environment, shard, cluster)
              /
              sum(
                gitlab:gcp_subnet_max_ips{environment="{{ $labels.environment }}"} * on (subnet) group_right gitlab:cluster:subnet:mapping{environment="{{ $labels.environment }}"}
              ) by (env, environment, shard, cluster)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_node_ips",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_node_ips",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Kube Pool Max Node Limit resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Pool Max Node Limit resource:

        A GKE kubernetes node pool is close to it's maximum number of nodes.

        The maximum is defined in terraform, via the `max_node_count` field of a node pool. The limit is per-zone, so for single zone clusters the number of nodes will match the limit, for regional clusters, the limit is multiplied by the number of zones the cluster is deployed over.
      grafana_dashboard_id: alerts-sat_kube_pool_max_nodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pool_max_nodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1686893332"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, cluster, label_pool, shard) (
          clamp_min(
            clamp_max(
              count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
                kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              / on(cluster, env, environment, label_pool) group_left() (
                label_replace(
                  terraform_report_google_cluster_node_pool_max_node_count,
                  "label_pool", "$0", "pool_name", ".*"
                )
                * on(cluster, env, environment) group_left()
                count by (cluster, env, environment) (
                  group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                    kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_pool_max_nodes",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_pool_max_nodes",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Average CPU Utilization on Postgres Primary Instance resource of
        the {{ $labels.type }} service ({{ $labels.stage }} stage) has a saturation
        exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization on Postgres Primary Instance resource:

        Average CPU utilization across all cores on the Postgres primary instance.
      grafana_dashboard_id: alerts-sat_pg_primary_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_primary_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3989464622"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (
                1
                -
                (
                  rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                  and on(fqdn)
                  pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="pg_primary_cpu",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_primary_cpu",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Postgres Autovacuum Activity (non-sampled) resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Postgres Autovacuum Activity (non-sampled) resource:

        This measures the total amount of time spent each day by autovacuum workers, as a percentage of total autovacuum capacity.

        This resource uses the `auto_vacuum_elapsed_seconds` value logged by the autovacuum worker, and aggregates this across all autovacuum jobs. In the case that there are 10 autovacuum workers, the total capacity is 10-days worth of autovacuum time per day.

        Once the system is performing 10 days worth of autovacuum per day, the capacity will be saturated.

        This resource is primarily intended to be used for long-term capacity planning.
      grafana_dashboard_id: alerts-sat_pg_vacuum_activity_v2
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_vacuum_activity_v2?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3412018865"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              avg by (env, environment, tier, type, stage, shard) (
                pg_settings_autovacuum_max_workers{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="ops",monitor="global"}
  - alert: component_saturation_slo_out_of_bounds
    for: 5m
    annotations:
      title: The Walsender CPU Saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Walsender CPU Saturation resource:

        This saturation metric measures the total amount of time that the primary postgres instance is spending sending WAL segments to replicas. It is expressed as a percentage of all CPU available on the primary postgres instance.

        The more replicas connected, the higher this metric will be.

        Since it's expressed as a percentage of all CPU, this should always remain low, since the CPU primarily needs to be available for handling SQL statements.
      grafana_dashboard_id: alerts-sat_pg_walsender_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_walsender_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1879384722"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                sum by(env, environment, tier, type, stage, shard, fqdn) (
                  rate(namedprocess_namegroup_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                  and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
                /
                count by (env, environment, tier, type, stage, shard, fqdn) (
                  node_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", mode="idle"} and on(fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="ops",monitor="global"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="ops",monitor="global"}
